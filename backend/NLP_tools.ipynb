{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5KRLS82Y_PX"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.4.24)\n",
            "Requirement already satisfied: joblib in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jerry\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\jerry\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKnBD3tLY844",
        "outputId": "5959a8d3-d449-4655-f2f0-0dd81ed20b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['An',\n",
              " 'HTTP',\n",
              " 'request',\n",
              " 'is',\n",
              " 'made',\n",
              " 'by',\n",
              " 'a',\n",
              " 'client',\n",
              " ',',\n",
              " 'to',\n",
              " 'a',\n",
              " 'named',\n",
              " 'host',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'located',\n",
              " 'on',\n",
              " 'a',\n",
              " 'server',\n",
              " '.',\n",
              " 'The',\n",
              " 'aim',\n",
              " 'of',\n",
              " 'the',\n",
              " 'request',\n",
              " 'is',\n",
              " 'to',\n",
              " 'access',\n",
              " 'a',\n",
              " 'resource',\n",
              " 'on',\n",
              " 'the',\n",
              " 'server',\n",
              " '.',\n",
              " 'To',\n",
              " 'make',\n",
              " 'the',\n",
              " 'request',\n",
              " ',',\n",
              " 'the',\n",
              " 'client',\n",
              " 'uses',\n",
              " 'components',\n",
              " 'of',\n",
              " 'a',\n",
              " 'URL',\n",
              " '(',\n",
              " 'Uniform',\n",
              " 'Resource',\n",
              " 'Locator',\n",
              " ')',\n",
              " ',',\n",
              " 'which',\n",
              " 'includes',\n",
              " 'the',\n",
              " 'information',\n",
              " 'needed',\n",
              " 'to',\n",
              " 'access',\n",
              " 'the',\n",
              " 'resource',\n",
              " '.']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import module for tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# input the text\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "word_tokenize(text)\n",
        "# Challenge: on first try, you may not be run into errors \"resource_not_found\". Attempt to solve this error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ivc3KeLa6HN"
      },
      "outputs": [],
      "source": [
        "# Mission: assign the tokens to an array named 'tokens'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3NYJs0Bb9-J"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u3KytuqcIz0",
        "outputId": "980ad3bf-98f9-45ea-df62-9b368327b558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rocks : rock\n"
          ]
        }
      ],
      "source": [
        "#import the module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# initiate the lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "# Challenge: on first try, you may not be run into errors \"resource_not_found\". Attempt to solve this error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGmVAaP8cZlz",
        "outputId": "3701af05-ce53-4188-c6cc-e0070337a5b6"
      },
      "outputs": [],
      "source": [
        "# Mission: Lemmatize all the words in the tokens array\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xka1cCgKeYfb"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rS44PQAQcv0Z",
        "outputId": "0d53ddda-97bd-4c86-e8d7-79632796821f"
      },
      "outputs": [],
      "source": [
        "# import the module\n",
        "from nltk.corpus import stopwords\n",
        "#assign to globally set stopwords to a local set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#filter the stopwords from the token\n",
        "filtered_tokens = [token for token in new_tokens if not token.lower() in stop_words]\n",
        "\n",
        "print(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('BuildingBloCS', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('yearly', 'JJ'), ('event', 'NN'), ('that', 'WDT'), ('teaches', 'VBZ'), ('student', 'NN'), ('computing', 'VBG'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"BuildingBloCS is a yearly event that teaches student computing.\"\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence:\n",
        "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
        "# now add your own text to tag it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "USING RAKE ALGORITHM TO EXTRACT KEYWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TO INSTALL RAKE MODULE\n",
        "!pip install rake-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['uniform resource locator ),',\n",
              " 'client uses components',\n",
              " 'named host',\n",
              " 'information needed',\n",
              " 'http request',\n",
              " 'resource',\n",
              " 'resource',\n",
              " 'client',\n",
              " 'request',\n",
              " 'request',\n",
              " 'url',\n",
              " 'server',\n",
              " 'server',\n",
              " 'make',\n",
              " 'made',\n",
              " 'located',\n",
              " 'includes',\n",
              " 'aim',\n",
              " 'access',\n",
              " 'access']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rake_nltk import Rake\n",
        "r = Rake()\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "r.extract_keywords_from_text(text)\n",
        "r.get_ranked_phrases()\n",
        "\n",
        "# Mission: One of the results this algorithm outputs is 'uniform resource locator )'. \n",
        "# Clean the text by removing punctuation marks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytextrank to extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install spacy\n",
        "!pip3 install pytextrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uniform Resource Locator\n",
            "a server\n",
            "the server\n",
            "components\n",
            "a resource\n",
            "the resource\n",
            "a named host\n",
            "the information\n",
            "An HTTP request\n",
            "a URL\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "# example text\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "# load a spaCy model, depending on language, scale, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# add PyTextRank to the spaCy pipeline\n",
        "nlp.add_pipe(\"textrank\")\n",
        "doc = nlp(text)\n",
        "# examine the top-ranked phrases in the document\n",
        "for phrase in doc._.phrases[:10]:\n",
        "    print(phrase.text)\n",
        "\n",
        "# there are multiple results with 'a' as part of the keyphrase,\n",
        "# can we possible remove them for they keywords to be better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementation for the TRA. \n",
        "Using a technique you have learnt just now OR using a new method you have found\n",
        "Design a function that takes in a bunch of sentences, and return a list with \n",
        "1. the sentence with the keywords replaced with blanks '_'\n",
        "2. the keywords\n",
        "eg. The mitochondria is the powerhouse of the cell\n",
        "returns\n",
        "['The _______ is the _______ of the cell', ['mitochondria','powerhouse']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP tools",
      "provenance": []
    },
    "interpreter": {
      "hash": "cdac3f338c4a9266f15b7211e0fd6aeed3fb6bd8b14dd7217ff2a9498d21b47d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
